{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b54e05c",
   "metadata": {},
   "source": [
    "The code below scrapes zipped pgn files from PGN Mentor then downloads them to `.` and extracts them `./pgns/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e994e335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import requests\n",
    "\n",
    "from time import time, sleep\n",
    "from pprint import pprint\n",
    "from zipfile import ZipFile, BadZipFile\n",
    "from datetime import timedelta\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup as BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad3da9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PGN_SRC_URL = 'https://www.pgnmentor.com'\n",
    "PGN_FILES_URL = f'{PGN_SRC_URL}/files.html'\n",
    "\n",
    "#PGN_SRC_CACHE_FNAME = 'main_png_site.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "13a3b43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object that downloads a file from the internet if it is not present\n",
    "# on disk or (optionally) if it is \"old\", as defined by the user.\n",
    "class WebFile():\n",
    "    # @url: web url\n",
    "    # @max_age: datetime.timedelta object (ex: timedelta(days=20))\n",
    "    # @filename: filename to write to disk; the class will find a\n",
    "    #            filename if this is empty\n",
    "    # @force: will force a download to disk if True\n",
    "    def __init__(self, url, max_age=None, filename=None, force=False):\n",
    "        self.url = url\n",
    "        self.max_age = max_age\n",
    "        self.contents = None\n",
    "        \n",
    "        if filename:\n",
    "            self.file = filename\n",
    "        else:\n",
    "            self.file = self.url_to_filename(url)\n",
    "            \n",
    "        self.initialize_file(force)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f'{self.file} ({self.getAge()}/{self.max_age})'\n",
    "\n",
    "    # Converts a url to a file name. Some guess work is involved in\n",
    "    # creating an extension for the file name, which might do more\n",
    "    # harm than good sometimes.\n",
    "    def url_to_filename(self, url):\n",
    "        basename = os.path.basename(url)\n",
    "        if '.' not in basename:\n",
    "            basename = f'{basename}.html'\n",
    "        return basename\n",
    "\n",
    "    # Makes sure file is on disk and up to date\n",
    "    def initialize_file(self, force=False):\n",
    "        if force or \\\n",
    "           not os.path.exists(self.file) or \\\n",
    "           self.isOutOfDate():\n",
    "            self.fetch()\n",
    "\n",
    "    # Downloads file to disk\n",
    "    def fetch(self):\n",
    "        r = requests.get(self.url, stream=True)\n",
    "        with open(self.file, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=1024):\n",
    "                f.write(chunk)\n",
    "        \n",
    "    # Returns a file's age in the form of a timedelta value\n",
    "    # Will raise an error if the file isn't on disk\n",
    "    def getAge(self):\n",
    "        diff = time() - os.path.getmtime(self.file)\n",
    "        return timedelta(seconds=diff)\n",
    "        \n",
    "    # Compares current time and file's last modification time\n",
    "    def isOutOfDate(self):\n",
    "        if not self.max_age:\n",
    "            return False\n",
    "\n",
    "        if self.getAge() > self.max_age:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    # Reads whole file into memory if not already in memory\n",
    "    def read(self):\n",
    "        if not self.contents:\n",
    "            with open(self.file) as f:\n",
    "                self.contents = f.read()\n",
    "        return self.contents\n",
    "\n",
    "    # Overwrites existing file with prettified html output (not general purpose)\n",
    "    def _prettify(self):\n",
    "        soup = BS(self.read())\n",
    "        self.contents = soup.prettify()\n",
    "        with open(self.file, 'w') as f:\n",
    "            f.write(self.contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "77d3c5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to facilitate downloading and extracting files from PGNMentor\n",
    "class PGNMentor():\n",
    "    EXTRACT_DIR = 'pgns'\n",
    "\n",
    "    def __init__(self):\n",
    "        wf = WebFile(PGN_FILES_URL, max_age=timedelta(days=5))\n",
    "        #wf._prettify()\n",
    "        self.soup = BS(wf.read())\n",
    "        \n",
    "        self.files = [] # populated in load_all\n",
    "        \n",
    "    # input a filename and the url will be determined and fetched\n",
    "    # WARNING: every file has to be downloaded for this to happen currently\n",
    "    def refreshOne(self, name):\n",
    "        if not self.files:\n",
    "            self.load_all()\n",
    "\n",
    "        fnames = [wf.file for wf in self.files]\n",
    "        if name not in fnames:\n",
    "            raise FileNotFoundError(name)\n",
    "            \n",
    "        wf = self.files[fnames.index(name)]\n",
    "        wf.fetch()\n",
    "\n",
    "    # returns list of urls found ex: ['players/Philidor.zip', ...]\n",
    "    def get_all_base_urls(self):\n",
    "        ret = set()\n",
    "        for tag in self.soup.find_all('a'):\n",
    "            try:\n",
    "                ret.add(tag['href'])\n",
    "            except KeyError:\n",
    "                continue\n",
    "        return ret\n",
    "    \n",
    "    def get_zip_urls(self):\n",
    "        ret = set()\n",
    "        for url in self.get_all_base_urls():\n",
    "            if url.endswith('.zip'):\n",
    "                ret.add(f'{PGN_SRC_URL}/{url}')\n",
    "        return ret\n",
    "    \n",
    "    def get_opening_urls(self):\n",
    "        ret = set()\n",
    "        for url in self.get_all_base_urls():\n",
    "            if url.startswith('openings') and url.endswith('.zip'):\n",
    "                ret.add(f'{PGN_SRC_URL}/{url}')\n",
    "        return ret\n",
    "    \n",
    "    def get_player_urls(self):\n",
    "        ret = set()\n",
    "        for url in self.get_all_base_urls():\n",
    "            if url.startswith('players') and url.endswith('.zip'):\n",
    "                ret.add(f'{PGN_SRC_URL}/{url}')\n",
    "        return ret\n",
    "        \n",
    "    # Download files from the site if they're not on disk or if they are old.\n",
    "    def _load_urls(self, urls, sleep_len=2, quiet=True):\n",
    "        start_time = time()\n",
    "\n",
    "        for url in urls:\n",
    "            wfile = WebFile(url, max_age=timedelta(days=90))\n",
    "            \n",
    "            # Sleep if we accessed the web and updated the file\n",
    "            if wfile.getAge() < timedelta(seconds=10):\n",
    "                sleep(sleep_len)\n",
    "\n",
    "            self.files.append(wfile)\n",
    "            if not quiet:\n",
    "                print('.', end='')\n",
    "        if not quiet:\n",
    "            print()\n",
    "\n",
    "        diff = time() - start_time\n",
    "        if not quiet:\n",
    "            print(f'Elapsed time: {int(diff)}s')\n",
    "            \n",
    "    def load_all(self, **kwargs):\n",
    "        return self._load_urls(self.get_zip_urls(), **kwargs)\n",
    "            \n",
    "    def load_openings(self, **kwargs):\n",
    "        return self._load_urls(self.get_opening_urls(), **kwargs)\n",
    "            \n",
    "    def load_players(self, **kwargs):\n",
    "        return self._load_urls(self.get_player_urls(), **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def _unzip(zip_filename):\n",
    "        try:\n",
    "            with ZipFile(zip_filename) as pgn_zip:\n",
    "                # Figure out what file we might want to extract. Don't use extractall\n",
    "                # because of the scary vulnerabilities it presents.\n",
    "                pgn_files = [fname for fname in pgn_zip.namelist() if fname.endswith('.pgn')]\n",
    "                for pgn_file in pgn_files: # should only be 1 file tbh\n",
    "                    pgn_zip.extract(pgn_file, path=PGNMentor.EXTRACT_DIR)\n",
    "        except:\n",
    "            # Don't raise any exception because there are just bad files sometimes\n",
    "            print(f'ERROR with: {zip_filename}')\n",
    "            return 0\n",
    "\n",
    "        return len(pgn_files)\n",
    "\n",
    "    # Extract all files listed on the website or in the current directory.\n",
    "    # WARNING: Silently overwrites existing files.\n",
    "    #\n",
    "    # @cpus: number of parallel unzips to run\n",
    "    # @quiet: display some info\n",
    "    # @file_list: extract from this list (self.files otherwise)\n",
    "    def extract_all(self, cpus=1, quiet=True, file_list=None):\n",
    "        try:\n",
    "            os.mkdir(PGNMentor.EXTRACT_DIR)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "\n",
    "        start_time = time()\n",
    "\n",
    "        # Get list of file names\n",
    "        if not file_list:\n",
    "            file_list = [f.file for f in self.files]\n",
    "            \n",
    "        # User hasn't called _load_urls yet, so just extract from the current\n",
    "        # directory.\n",
    "        if not file_list:\n",
    "            file_list = []\n",
    "            for root, dirs, files in os.walk('.'):\n",
    "                if root != '.':\n",
    "                    continue\n",
    "                file_list.extend((f for f in files if f.endswith('.zip')))\n",
    "                \n",
    "        p = Pool(cpus)\n",
    "\n",
    "        res = p.map(self._unzip, file_list)\n",
    "        \n",
    "        if not quiet:\n",
    "            diff = time() - start_time\n",
    "            print(f'Elapsed time: {int(diff)}s')\n",
    "            print(f'Extracted {sum(res)} .pgn files.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "315b13da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR with: ScotchGambit.zip\n",
      "Elapsed time: 7s\n",
      "Extracted 231 .pgn files.\n"
     ]
    }
   ],
   "source": [
    "p = PGNMentor()\n",
    "p.load_openings() # populates p.files and refreshes old files\n",
    "p.extract_all(quiet=False, cpus=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40378b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ee7ae331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR with: ScotchGambit.zip\n",
      "Elapsed time: 8s\n",
      "Extracted 476 .pgn files.\n"
     ]
    }
   ],
   "source": [
    "p = PGNMentor()\n",
    "p.load_all()\n",
    "#p.refreshOne('ScotchGambit.zip')\n",
    "p.extract_all(quiet=False, cpus=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49937ff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b33dc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
